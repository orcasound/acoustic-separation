# Spleeter
Spleeter uses a U-Net architecture for source separation. U-Nets input a spectrogram and perform a series of 2D convolutions, each of which producing an encoding of a smaller and smaller representation of the input. The small representation at the centre is then scaled back up by decoding with the same number of 2D deconvolutional layers (sometimes called transpose convolution), each of which corresponds to the shape of one of the convolutional encoding layers. Each of the encoding layers is concatenated to the corresponding decoding layers.<br/>
The final mask is multiplied by the input mixture and the loss is taken between the ground truth source spectrogram and mixture spectrogram with the estimated mask applied, as per usual.<br/>
Because the U-Net is convolutional, it must process a spectrogram that has a fixed shape. In other words, an audio signal must be broken up into spectrograms with the same number of time and frequency dimensions that the U-Net was trained with.
![U-net]<img src="/Deep-Learning-Separation/assets/U-net.png" width="500" height="400">

## Loss Function-
The loss function used to train the model is the L<sub>1,1</sub> norm of the difference of the target spectrogram and the masked input spectrogram:<br/>
L(X,Y ; Θ) = ||f(X,Θ)⊙ X - Y ||<sub>1,1</sub><br/>
where f(X, Θ) is the output of the network model applied to the input X with parameters Θ – that is the mask generated by the model.

You can find the original paper about the U-Net architecture here-https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf

## Validation Loss Curves-
<img src="/Deep-Learning-Separation/assets/absolute%20difference.png" width="600" height="400">
<img src="/Deep-Learning-Separation/assets/vocals.png" width="600" height="400">
<img src="/Deep-Learning-Separation/assets/accompaniment.png" width="600" height="400">

## Evaluation Metrics-

Evaluation Metrics                        | Value
----------------------------------------- | -------------
Mean Absolute Difference                  | 0.4626
Mean Absolute Difference (vocals)         | 0.234
Mean Absolute Difference (accompaniment)  | 0.2286


## How to use the fine-tuned model?
One can use the fine-tuned 2stems spleeter model to separate orca vocalizations from the audio. They can create an instance of the ‘Separator’ object defined in the Python file- ‘spleeter_separator.py’ which takes the location of the json file containing model paramters, and the path to directory containing the model checkpoint as arguments. One of its member functions, ‘return_source_directory’ can be called which returns a dictionary containing names of the sources and waveforms as numpy arrays corresponding to those sources. This function takes the waveform (numpy array) and sample rate as arguments.
The fine-tuned model can be accessed from [here](https://drive.google.com/drive/u/3/folders/1dQFwODO-pIYMax55gq7q6OKtlDkeN5Rz).
You can try out the fine-tuned spleeter model on [Google colab](https://colab.research.google.com/drive/1ijn3lBymWftxfWMEjGimgiG1r89XFm2x?authuser=3#scrollTo=j2ISw4z3ZDge) without installing anything on your system.
